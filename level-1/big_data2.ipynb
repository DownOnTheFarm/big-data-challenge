{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"big_data2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOETJqaoFjVdRhN6/vChAuW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":919},"id":"4zFH880DHevT","executionInfo":{"status":"error","timestamp":1635042370997,"user_tz":420,"elapsed":13651,"user":{"displayName":"Farm Family","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-7H4dEjbdHfMPzyCi9WNrjLs0CCFpZKCysCNG=s64","userId":"06384183235175568783"}},"outputId":"caf0a357-b31e-478b-b1c3-fa03997aa60f"},"source":["import os\n","# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.0.1'\n","spark_version = 'spark-3.0.3'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connected to cloud.r-proje\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [630 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,396 kB]\n","Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,434 kB]\n","Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [69.5 kB]\n","Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [753 kB]\n","Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,809 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [665 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,213 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,835 kB]\n","Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [926 kB]\n","Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Fetched 14.1 MB in 5s (2,784 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.3-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"]},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1bf6d6773c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Start a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         raise Exception(\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"]}]},{"cell_type":"code","metadata":{"id":"RiCDhJ5WHlyf"},"source":["!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGqyhcpvHpRy"},"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkfC38t3HvZm"},"source":["# Load Gift Card reviews data from s3.amazonaws.com\n","from pyspark import SparkFiles\n","url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz\"\n","\n","spark.sparkContext.addFile(url)\n","gift_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Gift_Card_v1_00.tsv.gz\"), sep=\"\\t\", header=True)\n","gift_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dG9-ZII7H7zN"},"source":["# Outputting the number of rows\n","gift_df.count()\n","\n","# Removed duplicate rows\n","gift_df = gift_df.dropDuplicates()\n","gift_df.count()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtKy6KlpIHjn"},"source":["# Keep and rename necessary columns\n","review_id_df = gift_df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"review_date\"])\n","review_id_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gm0i4WH6IMET"},"source":["# For Products table\n","products_df = gift_df.select([\"product_id\", \"product_title\"])\n","products_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9a-IE2FIL9s"},"source":["# For Customers table\n","customers_df = gift_df.groupby(\"customer_id\").agg({\n","    \"customer_id\": \"count\"\n","    }).withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n","customers_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ytzp6CVILok"},"source":["# For Review table\n","reviews_df = gift_df.select([\"review_id\", \"review_headline\", \"review_body\"])\n","reviews_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjseJ3pEJGpz"},"source":["# For Vine table\n","vine_df = gift_df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n","vine_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5F-FC3IJLn8"},"source":["# Configure settings for RDS\n","mode = \"append\"\n","jdbc_url=\"jdbc:postgresql://<connection string>:5432/<database-name>\"\n","config = {\"user\":\"postgres\", \n","          \"password\": \"<password>\", \n","          \"driver\":\"org.postgresql.Driver\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90I-yOvgJU8a"},"source":["# Write review_id_df to table in RDS\n","review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UC-GJunJbAG"},"source":["# Write products_df to table in RDS\n","products_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKZxJjVjJa47"},"source":["# Write customers_df to table in RDS\n","customers_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUh-7UqGJawc"},"source":["# Write vine_df to table in RDS\n","vine_df.write.jdbc(url=jdbc_url, table='vines', mode=mode, properties=config)"],"execution_count":null,"outputs":[]}]}